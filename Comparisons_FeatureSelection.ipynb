{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we improve anomaly detection results using feature selection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at the isolation forest algorithm. For this comparison, we will keep the max_sample parameter constant at 2000. See the file IsolationForest_Plots to see why this value was chosen. In order to understand how feature selection affects the effectiveness of this algorithm, we will compare recall, precision, and f1 scores for different subsets of the KDDcup99 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import numpy as np\n",
    "\n",
    "def calculations(actual, predictions):\n",
    "    r = recall_score(actual, predictions, pos_label = -1)\n",
    "    p = precision_score(actual, predictions, pos_label = -1)\n",
    "    f = f1_score(actual, predictions, pos_label = -1)\n",
    "    print('Recall =', r, '\\nPrecision =', p, '\\nf1 =', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "kdd99_data = fetch_kddcup99(subset='http')\n",
    "X_http = kdd99_data['data']\n",
    "y_http = kdd99_data['target']\n",
    "y_http[y_http == b'normal.'] = 1\n",
    "y_http[y_http != 1] = -1\n",
    "y_http = np.int64(y_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = IsolationForest(max_samples= 2000, random_state=42)\n",
    "isof.fit(X_http)\n",
    "anomaly_predictions_http = isof.predict(X_http)\n",
    "anomaly_predictions_http = np.array(anomaly_predictions_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 1.0 \n",
      "Precision = 0.5189100305379375 \n",
      "f1 = 0.6832663161150634\n"
     ]
    }
   ],
   "source": [
    "calculations(y_http, anomaly_predictions_http)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running isolation forest on the http subset of the KDDcup99 dataset gives the results for recall, precision, and f1 score above. It can be seen that all anomalies were detected, however there were some normal points that were labelled as anomalous as well. This means that 100% of the anomalies present in the data were found, but of the points labelled as anomalous only about 52% of them were corretly labelled. Consequently, this test was 68% effective in detecting anomalies on the http subset. It is important to note that only 3 features are considered in the http subset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens when we use a randomly selected subset of the data. Because the original dataset is much too large, it is necessary that we find a random sample that has a similar number of instances to http. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "kdd99_data = fetch_kddcup99()\n",
    "X = kdd99_data['data']\n",
    "y = kdd99_data['target']\n",
    "y[y == b'normal.'] = 1\n",
    "y[y != 1] = -1\n",
    "y = np.int64(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59000, 41)\n",
      "(59000,)\n"
     ]
    }
   ],
   "source": [
    "sample_indices = np.random.choice(range(len(y)), 59000)\n",
    "X_sample = X[sample_indices,:]\n",
    "y_sample=y[sample_indices]\n",
    "print(X_sample.shape)\n",
    "print(y_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample of 59000 from the original dataset has 41 features. Some of these features are categorical, while others are numerical. This can lead to a more complex problem, so for the sake of simplicity the categorical features will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59000, 38)\n",
      "(59000,)\n"
     ]
    }
   ],
   "source": [
    "X_num_sample = np.delete(X_sample,[1,2,3],1)\n",
    "print(X_num_sample.shape)\n",
    "print(y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = IsolationForest(max_samples= 2000, random_state=42)\n",
    "isof.fit(X_num_sample)\n",
    "anomaly_predictions = isof.predict(X_num_sample)\n",
    "anomaly_predictions = np.array(anomaly_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.024347973115779686 \n",
      "Precision = 0.48040033361134277 \n",
      "f1 = 0.0463469584808497\n"
     ]
    }
   ],
   "source": [
    "calculations(y_sample, anomaly_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all 38 numerical features impaired our results. While the precision score did not change much, the recall score dropped significantly. Only about 3% of existing anomalies were detected and thus the accuracy percentage suffers as well. Let's see if we can improve these results by using the scikit-learn feature selection method SelectKBest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "kdd99_data = fetch_kddcup99()\n",
    "X_feature = kdd99_data['data']\n",
    "y_feature = kdd99_data['target']\n",
    "y_feature[y_feature == b'normal.'] = 1\n",
    "y_feature[y_feature != 1] = -1\n",
    "y_feature = np.int64(y_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59000, 41)\n",
      "(59000,)\n"
     ]
    }
   ],
   "source": [
    "sample_indices_feat = np.random.choice(range(len(y_feature)), 59000)\n",
    "X_feat_sample = X_feature[sample_indices_feat,:]\n",
    "y_feat_sample=y_feature[sample_indices_feat]\n",
    "print(X_feat_sample.shape)\n",
    "print(y_feat_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59000, 38)\n",
      "(59000,)\n"
     ]
    }
   ],
   "source": [
    "X_num = np.delete(X_feat_sample,[1,2,3],1)\n",
    "print(X_num.shape)\n",
    "print(y_feat_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kajia\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [ 5 10 16 17] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "c:\\users\\kajia\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "fs = SelectKBest(score_func = f_classif, k = 3)\n",
    "X_selected = fs.fit_transform(X_num,y_feat_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = IsolationForest(max_samples= 2000, random_state=42)\n",
    "isof.fit(X_selected)\n",
    "anomaly_predictions_feature = isof.predict(X_selected)\n",
    "anomaly_predictions_feature = np.array(anomaly_predictions_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.1062814123188863 \n",
      "Precision = 0.35697132363328155 \n",
      "f1 = 0.16379575764450421\n"
     ]
    }
   ],
   "source": [
    "calculations(y_feat_sample, anomaly_predictions_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http Subset Results:\n",
      "Recall = 1.0 \n",
      "Precision = 0.5189100305379375 \n",
      "f1 = 0.6832663161150634\n",
      "\n",
      "\n",
      "Random Sample Without Feature Selection Results:\n",
      "Recall = 0.024347973115779686 \n",
      "Precision = 0.48040033361134277 \n",
      "f1 = 0.0463469584808497\n",
      "\n",
      "\n",
      "Feature Selection Results:\n",
      "Recall = 0.1062814123188863 \n",
      "Precision = 0.35697132363328155 \n",
      "f1 = 0.16379575764450421\n"
     ]
    }
   ],
   "source": [
    "print('http Subset Results:')\n",
    "calculations(y_http, anomaly_predictions_http)\n",
    "print('\\n')\n",
    "print('Random Sample Without Feature Selection Results:')\n",
    "calculations(y_sample, anomaly_predictions)\n",
    "print('\\n')\n",
    "print('Feature Selection Results:')\n",
    "calculations(y_feat_sample, anomaly_predictions_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all 38 features without dimensionality reduction proves to be an ineffective approach. While selecting features may have slightly improved results, the accuracy is still lacking. This particular experiment shows that the http subset gives the most accurate results. However, that could entirely be from the feature selection method itself. It is possible that other feature selection methods would perform better in finding relationships between the data and target variables. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
